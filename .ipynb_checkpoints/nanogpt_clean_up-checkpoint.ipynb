{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "# !pip install torch numpy transformers datasets tiktoken wandb tqdm\n",
    "# !pip install tiktoken\n",
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "from typing import Tuple, Callable, Dict, List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(input_path: str) -> Tuple[str, int]:\n",
    "    # Loading training text data\n",
    "    with open(input_path, \"r\", encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    print(f'Total number of characters = {len(text)}')\n",
    "    chars = sorted(list(set(text)))\n",
    "    vocab_size = len(chars)\n",
    "    print(f'Total number of unique characters = {vocab_size}')\n",
    "    print(f\"Characters = {''.join(chars)}\")\n",
    "    return text, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters = 1115394\n",
      "Total number of unique characters = 65\n",
      "Characters = \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "input_path = 'input.txt'\n",
    "text, vocab_size = load_data(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_tokenizer(tokenizer_type: str) -> Tuple[Callable, Callable, int]:\n",
    "    '''choose_tokenizer should return a tuple of two callable objects: tokenizer and decoder'''\n",
    "    test_phrase = 'Hello Lord!'\n",
    "    if tokenizer_type == 'simple':\n",
    "        chars = sorted(list(set(text)))\n",
    "        vocab_size = len(chars)\n",
    "        stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "        itos = {i: ch for i, ch in enumerate(chars)}\n",
    "        encoder = lambda x: [stoi[ch] for ch in x]\n",
    "        decoder = lambda x: ''.join([itos[i] for i in x])\n",
    "        print(f'Number of vocabulary = {vocab_size}')\n",
    "        print(f'Encoding {test_phrase}: {encoder(test_phrase)}')\n",
    "        print(f'Decoding {test_phrase}: {decoder(encoder(test_phrase))}')\n",
    "    elif tokenizer_type == 'sentencepiece':\n",
    "        import sentencepiece as spm\n",
    "        vocab_size = 1000\n",
    "        params = (''.join(['--input=', input_path, ' ', '--model_prefix=spm ', '--vocab_size=', str(vocab_size), ' ']))\n",
    "        spm.SentencePieceTrainer.Train(params)\n",
    "        sp = spm.SentencePieceProcessor()\n",
    "        sp.Load('spm.model')\n",
    "        encoder = sp.EncodeAsIds\n",
    "        decoder = sp.DecodeIds\n",
    "        print(f'Number of vocabulary = {vocab_size}')\n",
    "        print(f\"Encoding {test_phrase} as pieces: {sp.EncodeAsPieces(test_phrase)}\")\n",
    "        print(f\"Encoding {test_phrase} as IDs: {encoder(test_phrase)}\")\n",
    "        print(f\"Decoding IDs: {decoder(encoder(test_phrase))}\")\n",
    "    elif tokenizer_type == 'tiktoken':\n",
    "        import tiktoken\n",
    "        enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        vocab_size = enc.n_vocab\n",
    "        encoder = enc.encode\n",
    "        decoder = enc.decode\n",
    "        print(f'Number of vocabulary = {vocab_size}')\n",
    "        print(f'Encoding {test_phrase}: {encoder(test_phrase)}')\n",
    "        print(f'Decoding {test_phrase}: {decoder(encoder(test_phrase))}')\n",
    "\n",
    "    return encoder, decoder, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vocabulary = 65\n",
      "Encoding Hello Lord!: [20, 43, 50, 50, 53, 1, 24, 53, 56, 42, 2]\n",
      "Decoding Hello Lord!: Hello Lord!\n",
      "Number of vocabulary = 1000\n",
      "Encoding Hello Lord! as pieces: ['▁He', 'll', 'o', '▁Lord', '!']\n",
      "Encoding Hello Lord! as IDs: [184, 65, 25, 338, 32]\n",
      "Decoding IDs: Hello Lord!\n",
      "Number of vocabulary = 100277\n",
      "Encoding Hello Lord!: [9906, 10425, 0]\n",
      "Decoding Hello Lord!: Hello Lord!\n"
     ]
    }
   ],
   "source": [
    "encoder1, decoder1, vocab_size1 = choose_tokenizer('simple')\n",
    "encoder2, decoder2, vocab_size2 = choose_tokenizer('sentencepiece')\n",
    "encoder3, decoder3, vocab_size3 = choose_tokenizer('tiktoken')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using simple tokenizer, there are 1115394 tokens, \n",
      "Using sentencepiece tokenizer, there are 407344 tokens, \n",
      "Using tiktoken tokenizer, there are 301829 tokens.\n",
      "--------\n",
      "Simple tokenizer: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43])\n",
      "First Citizen:\n",
      "Before we proce\n",
      "--------\n",
      "Sentencepiece tokenizer: tensor([298, 537,   6, 259, 280,  12,  81, 231, 107,  35, 384, 987,   3, 223,\n",
      "         33, 254,   7,  52,  65,   6,  78, 358,  21,  73,   3, 254,   7, 298,\n",
      "        537,   6])\n",
      "First Citizen: Before we proceed any further, hear me speak. All: Speak, speak. First Citizen:\n",
      "--------\n",
      "Tiktoken tokenizer: tensor([ 5451, 47317,   512, 10438,   584, 10570,   904,  4726,    11,  6865,\n",
      "          757,  6604,   382,  2460,   512, 96945,    11,  6604,   382,  5451,\n",
      "        47317,   512,  2675,   527,   682, 20250,  4856,   311,  2815,  1109])\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "data1 = torch.tensor(encoder1(text),dtype=torch.long)\n",
    "data2 = torch.tensor(encoder2(text),dtype=torch.long)\n",
    "data3 = torch.tensor(encoder3(text),dtype=torch.long)\n",
    "print(f'Using simple tokenizer, there are {data1.shape[0]} tokens, \\nUsing sentencepiece tokenizer, there are {data2.shape[0]} tokens, \\nUsing tiktoken tokenizer, there are {data3.shape[0]} tokens.\\n--------')\n",
    "sample_length = 30\n",
    "print(f'Simple tokenizer: {data1[:sample_length]}\\n{decoder1(data1[:sample_length].tolist())}\\n--------')\n",
    "print(f'Sentencepiece tokenizer: {data2[:sample_length]}\\n{decoder2(data2[:sample_length].tolist())}\\n--------')\n",
    "print(f'Tiktoken tokenizer: {data3[:sample_length]}\\n{decoder3(data3[:sample_length].tolist())}\\n--------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "n = 0.8 # train ratio\n",
    "train_data1 = data1[:int(n*len(data1))]\n",
    "val_data1 = data1[int(n*len(data1)):]\n",
    "\n",
    "train_data2 = data2[:int(n*len(data2))]\n",
    "val_data2 = data2[int(n*len(data2)):]\n",
    "\n",
    "train_data3 = data3[:int(n*len(data3))]\n",
    "val_data3 = data3[int(n*len(data3)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split: str, tokenizer: str, batch_size: int = 4, block_size: int = 8) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    if tokenizer == 'simple':\n",
    "        if split == 'train':\n",
    "            data = train_data1\n",
    "        else:\n",
    "            data = val_data1\n",
    "    elif tokenizer == 'sentencepiece':\n",
    "        if split == 'train':\n",
    "            data = train_data2\n",
    "        else:\n",
    "            data = val_data2\n",
    "    else:\n",
    "        if split == 'train':\n",
    "            data = train_data3\n",
    "        else:\n",
    "            data = val_data3\n",
    "    idx = torch.randint(0, data.size(0) - block_size, (batch_size,)) # select random starting indices to form a batch\n",
    "    x = torch.stack([data[i:i+block_size] for i in idx])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(xb: torch.Tensor, yb: torch.Tensor, encoder: Callable, decoder: Callable, block_size: int = 8, decode: bool = False) -> None:\n",
    "    # for each batch, there are 8 tokens in the input and target, but actually this contains 8 examples, corresponding to 8 different time steps, each with a target token that is shifted one position to the right\n",
    "    for b in range(xb.shape[0]): # batch dimension\n",
    "        for t in range(block_size): # time dimension\n",
    "            context = xb[b, :t+1]\n",
    "            target = yb[b,t]\n",
    "            print(f\"when input is {context.tolist()} the target: {target}\")\n",
    "            if decode == True:\n",
    "                print(f'context = {decoder(context.tolist())} \\ntarget = {decoder([target.tolist()])}')\n",
    "        print('\\n--------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape = torch.Size([4, 8])\n",
      "tensor([[58, 63,  8,  0,  0, 19, 24, 27],\n",
      "        [39, 59, 45, 46, 58,  1, 46, 43],\n",
      "        [49, 43, 57,  1, 53, 50, 42,  1],\n",
      "        [52, 41, 47, 43, 52, 58,  1, 56]])\n",
      "target shape = torch.Size([4, 8])\n",
      "tensor([[63,  8,  0,  0, 19, 24, 27, 33],\n",
      "        [59, 45, 46, 58,  1, 46, 43,  1],\n",
      "        [43, 57,  1, 53, 50, 42,  1, 46],\n",
      "        [41, 47, 43, 52, 58,  1, 56, 47]])\n",
      "input shape = torch.Size([4, 8])\n",
      "tensor([[ 29, 100,  68, 130,  31,   3, 442,  63],\n",
      "        [  9,  56, 979,   8,   3, 247, 106, 490],\n",
      "        [ 11,   8, 123, 923,  11,   5, 345,   3],\n",
      "        [ 61, 127,   4, 132,  12, 284,  14, 352]])\n",
      "target shape = torch.Size([4, 8])\n",
      "tensor([[100,  68, 130,  31,   3, 442,  63, 959],\n",
      "        [ 56, 979,   8,   3, 247, 106, 490,  27],\n",
      "        [  8, 123, 923,  11,   5, 345,   3,  88],\n",
      "        [127,   4, 132,  12, 284,  14, 352,  16]])\n",
      "input shape = torch.Size([4, 8])\n",
      "tensor([[ 1833,   499,   382,  5176,   785,   512,  1687,   527],\n",
      "        [  584, 22525,   539,   311, 23973,  1461,   382, 16041],\n",
      "        [  275, 50153,  1457,    11,   856,  4538,   345, 11087],\n",
      "        [33763,  3221,  6463, 43726,  1022,    11,   856, 82426]])\n",
      "target shape = torch.Size([4, 8])\n",
      "tensor([[  499,   382,  5176,   785,   512,  1687,   527, 12704],\n",
      "        [22525,   539,   311, 23973,  1461,   382, 16041, 47317],\n",
      "        [50153,  1457,    11,   856,  4538,   345, 11087,   311],\n",
      "        [ 3221,  6463, 43726,  1022,    11,   856, 82426, 38031]])\n",
      "----\n",
      "when input is [58] the target: 63\n",
      "when input is [58, 63] the target: 8\n",
      "when input is [58, 63, 8] the target: 0\n",
      "when input is [58, 63, 8, 0] the target: 0\n",
      "when input is [58, 63, 8, 0, 0] the target: 19\n",
      "when input is [58, 63, 8, 0, 0, 19] the target: 24\n",
      "when input is [58, 63, 8, 0, 0, 19, 24] the target: 27\n",
      "when input is [58, 63, 8, 0, 0, 19, 24, 27] the target: 33\n",
      "\n",
      "--------\n",
      "when input is [39] the target: 59\n",
      "when input is [39, 59] the target: 45\n",
      "when input is [39, 59, 45] the target: 46\n",
      "when input is [39, 59, 45, 46] the target: 58\n",
      "when input is [39, 59, 45, 46, 58] the target: 1\n",
      "when input is [39, 59, 45, 46, 58, 1] the target: 46\n",
      "when input is [39, 59, 45, 46, 58, 1, 46] the target: 43\n",
      "when input is [39, 59, 45, 46, 58, 1, 46, 43] the target: 1\n",
      "\n",
      "--------\n",
      "when input is [49] the target: 43\n",
      "when input is [49, 43] the target: 57\n",
      "when input is [49, 43, 57] the target: 1\n",
      "when input is [49, 43, 57, 1] the target: 53\n",
      "when input is [49, 43, 57, 1, 53] the target: 50\n",
      "when input is [49, 43, 57, 1, 53, 50] the target: 42\n",
      "when input is [49, 43, 57, 1, 53, 50, 42] the target: 1\n",
      "when input is [49, 43, 57, 1, 53, 50, 42, 1] the target: 46\n",
      "\n",
      "--------\n",
      "when input is [52] the target: 41\n",
      "when input is [52, 41] the target: 47\n",
      "when input is [52, 41, 47] the target: 43\n",
      "when input is [52, 41, 47, 43] the target: 52\n",
      "when input is [52, 41, 47, 43, 52] the target: 58\n",
      "when input is [52, 41, 47, 43, 52, 58] the target: 1\n",
      "when input is [52, 41, 47, 43, 52, 58, 1] the target: 56\n",
      "when input is [52, 41, 47, 43, 52, 58, 1, 56] the target: 47\n",
      "\n",
      "--------\n",
      "when input is [29] the target: 100\n",
      "when input is [29, 100] the target: 68\n",
      "when input is [29, 100, 68] the target: 130\n",
      "when input is [29, 100, 68, 130] the target: 31\n",
      "when input is [29, 100, 68, 130, 31] the target: 3\n",
      "when input is [29, 100, 68, 130, 31, 3] the target: 442\n",
      "when input is [29, 100, 68, 130, 31, 3, 442] the target: 63\n",
      "when input is [29, 100, 68, 130, 31, 3, 442, 63] the target: 959\n",
      "\n",
      "--------\n",
      "when input is [9] the target: 56\n",
      "when input is [9, 56] the target: 979\n",
      "when input is [9, 56, 979] the target: 8\n",
      "when input is [9, 56, 979, 8] the target: 3\n",
      "when input is [9, 56, 979, 8, 3] the target: 247\n",
      "when input is [9, 56, 979, 8, 3, 247] the target: 106\n",
      "when input is [9, 56, 979, 8, 3, 247, 106] the target: 490\n",
      "when input is [9, 56, 979, 8, 3, 247, 106, 490] the target: 27\n",
      "\n",
      "--------\n",
      "when input is [11] the target: 8\n",
      "when input is [11, 8] the target: 123\n",
      "when input is [11, 8, 123] the target: 923\n",
      "when input is [11, 8, 123, 923] the target: 11\n",
      "when input is [11, 8, 123, 923, 11] the target: 5\n",
      "when input is [11, 8, 123, 923, 11, 5] the target: 345\n",
      "when input is [11, 8, 123, 923, 11, 5, 345] the target: 3\n",
      "when input is [11, 8, 123, 923, 11, 5, 345, 3] the target: 88\n",
      "\n",
      "--------\n",
      "when input is [61] the target: 127\n",
      "when input is [61, 127] the target: 4\n",
      "when input is [61, 127, 4] the target: 132\n",
      "when input is [61, 127, 4, 132] the target: 12\n",
      "when input is [61, 127, 4, 132, 12] the target: 284\n",
      "when input is [61, 127, 4, 132, 12, 284] the target: 14\n",
      "when input is [61, 127, 4, 132, 12, 284, 14] the target: 352\n",
      "when input is [61, 127, 4, 132, 12, 284, 14, 352] the target: 16\n",
      "\n",
      "--------\n",
      "when input is [1833] the target: 499\n",
      "when input is [1833, 499] the target: 382\n",
      "when input is [1833, 499, 382] the target: 5176\n",
      "when input is [1833, 499, 382, 5176] the target: 785\n",
      "when input is [1833, 499, 382, 5176, 785] the target: 512\n",
      "when input is [1833, 499, 382, 5176, 785, 512] the target: 1687\n",
      "when input is [1833, 499, 382, 5176, 785, 512, 1687] the target: 527\n",
      "when input is [1833, 499, 382, 5176, 785, 512, 1687, 527] the target: 12704\n",
      "\n",
      "--------\n",
      "when input is [584] the target: 22525\n",
      "when input is [584, 22525] the target: 539\n",
      "when input is [584, 22525, 539] the target: 311\n",
      "when input is [584, 22525, 539, 311] the target: 23973\n",
      "when input is [584, 22525, 539, 311, 23973] the target: 1461\n",
      "when input is [584, 22525, 539, 311, 23973, 1461] the target: 382\n",
      "when input is [584, 22525, 539, 311, 23973, 1461, 382] the target: 16041\n",
      "when input is [584, 22525, 539, 311, 23973, 1461, 382, 16041] the target: 47317\n",
      "\n",
      "--------\n",
      "when input is [275] the target: 50153\n",
      "when input is [275, 50153] the target: 1457\n",
      "when input is [275, 50153, 1457] the target: 11\n",
      "when input is [275, 50153, 1457, 11] the target: 856\n",
      "when input is [275, 50153, 1457, 11, 856] the target: 4538\n",
      "when input is [275, 50153, 1457, 11, 856, 4538] the target: 345\n",
      "when input is [275, 50153, 1457, 11, 856, 4538, 345] the target: 11087\n",
      "when input is [275, 50153, 1457, 11, 856, 4538, 345, 11087] the target: 311\n",
      "\n",
      "--------\n",
      "when input is [33763] the target: 3221\n",
      "when input is [33763, 3221] the target: 6463\n",
      "when input is [33763, 3221, 6463] the target: 43726\n",
      "when input is [33763, 3221, 6463, 43726] the target: 1022\n",
      "when input is [33763, 3221, 6463, 43726, 1022] the target: 11\n",
      "when input is [33763, 3221, 6463, 43726, 1022, 11] the target: 856\n",
      "when input is [33763, 3221, 6463, 43726, 1022, 11, 856] the target: 82426\n",
      "when input is [33763, 3221, 6463, 43726, 1022, 11, 856, 82426] the target: 38031\n",
      "\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "block_size = 8\n",
    "xb1, yb1 = get_batch(split = 'train', tokenizer = 'simple')\n",
    "xb2, yb2 = get_batch(split = 'train', tokenizer = 'sentencepiece')\n",
    "xb3, yb3 = get_batch(split = 'train', tokenizer = 'tiktoken')\n",
    "\n",
    "print(f'input shape = {xb1.shape}')\n",
    "print(xb1) # input to the transformer\n",
    "print(f'target shape = {yb1.shape}')\n",
    "print(yb1)\n",
    "\n",
    "print(f'input shape = {xb2.shape}')\n",
    "print(xb2)\n",
    "print(f'target shape = {yb2.shape}')\n",
    "print(yb2)\n",
    "\n",
    "print(f'input shape = {xb3.shape}')\n",
    "print(xb3)\n",
    "print(f'target shape = {yb3.shape}')\n",
    "print(yb3)\n",
    "\n",
    "print('----')\n",
    "show_sample(xb1, yb1, encoder1, decoder1, block_size = 8, decode = False)\n",
    "show_sample(xb2, yb2, encoder2, decoder2, block_size = 8, decode = False)\n",
    "show_sample(xb3, yb3, encoder3, decoder3, block_size = 8, decode = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(\n",
    "    model: nn.Module, \n",
    "    eval_iters: int = 200, \n",
    "    tokenizer: str = 'simple', \n",
    "    batch_size: int = 32, \n",
    "    block_size: int = 8\n",
    ") -> Dict[str, float]:\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            xb, yb = get_batch(split, tokenizer = tokenizer, batch_size = batch_size, block_size = block_size)\n",
    "            _, loss = model(xb, yb)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim: int = 32, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        self.eps = eps\n",
    "    def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        xmean =x.mean(1, keepdim=True) # layer mean, if dim = 0, then it is batch mean for batch norm\n",
    "        xvar = x.var(1, keepdim=True) # layer variance, if dim = 0, then it is batch variance for batch norm\n",
    "        xhat = (x-xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta] # trainable parameters\n",
    "\n",
    "class Head(nn.Module): # this is one attention head\n",
    "    def __init__(self, head_size: int = 16, n_embd: int = 32, block_size: int = 8, dropout_r: float = 0.1):\n",
    "        super().__init__()\n",
    "        # hyperparameters\n",
    "        self.head_size = head_size\n",
    "        # layers\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout_r)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B,T,C = x.shape\n",
    "        #print(f'x shape = {x.shape}')\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        wei = q @ k.transpose(-2, -1) * self.head_size**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        return wei @ v\n",
    "\n",
    "# multi-head self-attention\n",
    "class MultiHead(nn.Module):\n",
    "    def __init__(self, head_size: int = 16, n_embd: int = 32, block_size: int = 8, n_heads: int = 8, dropout_r: float = 0.1):\n",
    "        super().__init__()\n",
    "        # layers\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embd, \n",
    "                                         block_size, dropout_r) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_heads*head_size, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout_r)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        x = self.dropout(self.proj(x))\n",
    "        return x\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd: int = 32, dropout_r: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "            nn.Dropout(dropout_r)\n",
    "        )\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, head_size: int = 16, n_embd: int = 32, block_size: int = 8, n_heads: int = 8, dropout_r: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.multi_head = MultiHead(head_size, n_embd, block_size, n_heads, dropout_r)\n",
    "        self.ff = FeedForward(n_embd, dropout_r)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.multi_head(self.ln1(x)) + x\n",
    "        x = self.ff(self.ln2(x)) + x\n",
    "        return x\n",
    "        \n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int = 65, n_embd: int = 32, block_size: int = 8, head_size: int = 16, n_heads: int = 8, n_blocks: int = 6, dropout_r: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd = n_embd, block_size = block_size, head_size = head_size, n_heads = n_heads, dropout_r = dropout_r) for _ in range(n_blocks)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size) # language model head\n",
    "\n",
    "    def forward(self, idx: torch.Tensor, targets: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        B, T = idx.shape\n",
    "        pos_embd = self.positional_embedding_table(torch.arange(T, device=idx.device)) # (T,n_embd)\n",
    "        # this is the embedded representation of the input, where idx is the input xb tensor\n",
    "        tok_embd = self.token_embedding_table(idx) # (B,T,n_embd)\n",
    "        embd = tok_embd + pos_embd # (B,T,n_embd), pytorch will broadcast pos_embd to (B,T,n_embd), C = n_embd\n",
    "        embd = self.blocks(embd) # (B,T,n_embd)\n",
    "        embd = self.ln_f(embd) # (B,T,n_embd)\n",
    "        logits = self.lm_head(embd) # (B,T,C), C = vocab_size = number of classes\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # reshape due to cross-entropy\n",
    "            targets = targets.view(B*T) # reshape due to cross-entropy\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.Tensor, max_new_tokens: int) -> torch.Tensor: # to generate new tokens\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens): # max_new_tokens is the maximum number of tokens to generate\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            # get the predictions\n",
    "            logits, _ = self(idx_cond) # loss is none here\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities for each class\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution using the probabilities\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram model training\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "head_size = 16 # n_embd//n_heads\n",
    "n_embd = 32\n",
    "n_heads = 8\n",
    "dropout_r = 0.1\n",
    "\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "max_iters = 5000\n",
    "learning_rate = 1e-3\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "def train(tokenizer = 'simple', vocab_size = vocab_size1):\n",
    "    model = BigramLanguageModel(vocab_size=vocab_size, n_embd=n_embd, block_size=block_size, \n",
    "                                head_size = head_size, n_heads = n_heads, dropout_r = dropout_r)\n",
    "    m = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "\n",
    "    for iter in range(max_iters):\n",
    "        if iter % eval_interval == 0:\n",
    "            losses = estimate_loss(model, eval_iters = eval_iters, tokenizer = tokenizer, \n",
    "                                   batch_size = batch_size, block_size = block_size)\n",
    "            print(f'Iter {iter}, train loss = {losses[\"train\"]:.4f}, val loss = {losses[\"val\"]:.4f}')\n",
    "\n",
    "        xb, yb = get_batch('train', tokenizer, batch_size, block_size)\n",
    "        logits, loss = m(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True) # If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, train loss = 4.3966, val loss = 4.3933\n",
      "Iter 500, train loss = 2.3737, val loss = 2.3804\n",
      "Iter 1000, train loss = 2.2312, val loss = 2.2625\n",
      "Iter 1500, train loss = 2.1477, val loss = 2.1814\n",
      "Iter 2000, train loss = 2.0971, val loss = 2.1596\n",
      "Iter 2500, train loss = 2.0547, val loss = 2.1229\n",
      "Iter 3000, train loss = 2.0090, val loss = 2.0948\n",
      "Iter 3500, train loss = 1.9883, val loss = 2.0825\n",
      "Iter 4000, train loss = 1.9755, val loss = 2.0776\n",
      "Iter 4500, train loss = 1.9389, val loss = 2.0511\n",
      "\n",
      "Rur parten: I ming yu; our anstogeraty,\n",
      "To notsme pralce to hat love\n",
      "shall my soselder I at brity to\n"
     ]
    }
   ],
   "source": [
    "m1 = train(tokenizer = 'simple', vocab_size = vocab_size1)\n",
    "# check the generated text after training\n",
    "example_x = torch.zeros((1, 1), dtype=torch.long, device = device)\n",
    "print(decoder1(m1.generate(idx = example_x, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, train loss = 7.0807, val loss = 7.0758\n",
      "Iter 500, train loss = 5.4188, val loss = 5.4972\n",
      "Iter 1000, train loss = 4.8801, val loss = 5.0981\n",
      "Iter 1500, train loss = 4.6675, val loss = 4.9566\n",
      "Iter 2000, train loss = 4.5488, val loss = 4.8549\n",
      "Iter 2500, train loss = 4.4249, val loss = 4.8327\n",
      "Iter 3000, train loss = 4.3620, val loss = 4.7415\n",
      "Iter 3500, train loss = 4.3085, val loss = 4.7563\n",
      "Iter 4000, train loss = 4.2745, val loss = 4.7262\n",
      "Iter 4500, train loss = 4.1842, val loss = 4.6817\n",
      " ⁇ ABETH: Ho this my face I fear of this prel, splest, Retitter you have muther, Or when I nacrifment son 'Tis would make. LORD VOLUMNIA: Hape from murder! how bring the father's pats will thates: To hear Fin away being Jight 'Tis not nightove trouarld'd which the humblur\n"
     ]
    }
   ],
   "source": [
    "m2 = train(tokenizer = 'sentencepiece', vocab_size = vocab_size2)\n",
    "# check the generated text after training\n",
    "example_x = torch.zeros((1, 1), dtype=torch.long, device = device)\n",
    "print(decoder2(m2.generate(idx = example_x, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, train loss = 11.6907, val loss = 11.6948\n",
      "Iter 500, train loss = 6.6760, val loss = 7.0683\n",
      "Iter 1000, train loss = 5.9611, val loss = 6.6715\n",
      "Iter 1500, train loss = 5.6093, val loss = 6.4514\n",
      "Iter 2000, train loss = 5.4087, val loss = 6.3321\n",
      "Iter 2500, train loss = 5.2206, val loss = 6.3148\n",
      "Iter 3000, train loss = 5.0945, val loss = 6.2945\n",
      "Iter 3500, train loss = 5.0143, val loss = 6.3112\n",
      "Iter 4000, train loss = 4.9372, val loss = 6.2748\n",
      "Iter 4500, train loss = 4.8775, val loss = 6.2482\n",
      "! O, up the infinite, you, let friends.\n",
      "Well, greatTriState condemn, you know.\n",
      "\n",
      "ANGELO:\n",
      "Hardam, my lord, only outemon! yet: painted\n",
      "A faithful royal prayer'd'd and co spider to no mortal, that, suitthacks,ft of all to himself.\n",
      "\n",
      "DUKE OF YORK:\n",
      "Hear, thanks, Marci shall\n",
      "Withqu crown: successful, with him.\n",
      "\n",
      "Second MurderCIUS:\n",
      "O the rest gone as word,\n",
      "Of craft no\n"
     ]
    }
   ],
   "source": [
    "m3 = train(tokenizer = 'tiktoken', vocab_size = vocab_size3)\n",
    "# check the generated text after training\n",
    "example_x = torch.zeros((1, 1), dtype=torch.long, device = device)\n",
    "print(decoder3(m3.generate(idx = example_x, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 153921\n",
      "Total number of parameters: 214696\n",
      "Total number of parameters: 6667701\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'Total number of parameters: {count_parameters(m1)}')\n",
    "print(f'Total number of parameters: {count_parameters(m2)}')\n",
    "print(f'Total number of parameters: {count_parameters(m3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try wordpiece tokenizer: pip install word-piece-tokenizer\n",
    "# from word_piece_tokenizer import WordPieceTokenizer\n",
    "# tokenizer = WordPieceTokenizer()\n",
    "# ids = tokenizer.tokenize('reading a storybook!')\n",
    "# tokens = tokenizer.convert_ids_to_tokens(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses may not be comparable due to different vocabulary sizes\n",
    "\n",
    "# larger vocabulary sizes also make training slower: The vocabulary size can significantly impact the training speed of a Transformer model in several ways:\n",
    "# - Embedding Layer Size: size of embedding matrix grows\n",
    "# - Softmax Computation: larger vocabulary means more computations per token\n",
    "# - Attention Mechanism: the overall computation per token includes the embedding lookups and softmax operations --> slower with larger vocab size\n",
    "\n",
    "# - mitigation: Subword Tokenization: \n",
    "# - Methods like Byte Pair Encoding (BPE) or WordPiece can reduce the effective vocabulary size by breaking words into smaller subword units. This helps in balancing between expressiveness and computational efficiency.\n",
    "# - Sampling-Based Softmax: Negative Sampling or Hierarchical Softmax can reduce the computational cost of the softmax function by approximating the full softmax\n",
    "# - Mixed-Precision Training: Using lower precision (like float16 instead of float32) for embeddings and other parameters can reduce memory usage and improve computational speed without significantly affecting model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
